{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78bb7df8",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "The training pipeline is composed by the following steps:\n",
    "\n",
    "- [0. Setup](#0.-Setup)\n",
    "- [1. Data extraction](#1.-Data-extraction)\n",
    "- [2. Data formatting](#2.-Data-formatting)\n",
    "- [3. Modeling](#3.-Modeling)\n",
    " - [3.1. Bag-of-words feature extraction](#3.1-Bag-of-words-feature-extraction)\n",
    "- [4. Model validation](#4.-Model-validation)\n",
    "- [5. Model exportation](#5.-Model-exportation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e251d8f1",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e36e038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916cf2b",
   "metadata": {},
   "source": [
    "## 1. Data extraction\n",
    "\n",
    "Loads a dataset with product data from a specified path available in the environment variable DATASET_PATH.\n",
    "Select only feature subset to reduce needed memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7802d6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "38000 products\n",
      "6 categories\n",
      "\n",
      "Test data:\n",
      "500 products\n",
      "6 categories\n",
      "\n",
      "Data extraction complete!\n"
     ]
    }
   ],
   "source": [
    "# Read training set\n",
    "products_train = pd.read_csv(os.environ['DATASET_PATH'])\n",
    "\n",
    "# Read test set\n",
    "products_test = pd.read_csv(os.environ['TEST_PATH'])\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(\"%d products\" % len(products_train))\n",
    "print(\"%d categories\" % len(products_train['category'].value_counts()))\n",
    "print()\n",
    "print(\"Test data:\")\n",
    "print(\"%d products\" % len(products_test))\n",
    "print(\"%d categories\" % len(products_test['category'].value_counts()))\n",
    "print()\n",
    "print('Data extraction complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9fe0f",
   "metadata": {},
   "source": [
    "## 2. Data formatting\n",
    "Processes the dataset to use it for training and validation.\n",
    "\n",
    "During experiments we used GridSearchCV class, which provided a cross-validation (5 k-fold) method to search the best set of features. Here we present only the best feature composition produced. The exploration path is documented in experiments.ipynb file.\n",
    "\n",
    "In order to reduce memory requirements and facilitate processing, we selected only the columns used during training. After that, to avoid pipeline execution problems, we discarded 60 rows that had missing values.\n",
    "\n",
    "During the experiments, we noticed that combining the text columns improved the model's performance. So we ended up combining the 'title', 'concataned_tag' and 'query' columns.\n",
    "\n",
    "In addition, in order to integrate information from three numeric columns ('price', 'weight', 'minimum_quantity') we used the k-means clustering algorithm to generate a new text feature ('kmeansPriceWeightMinimumQuantity') to be combined with the others. In this case, the best result was found when we defined the algorithm to run to find 23 clusters.\n",
    "\n",
    "It is worth mentioning that the dataset is clearly unbalanced. But balancing it did not improve the performance of the classifier. We think the reason is that the test set itself is unbalanced in the same distribution of training set. Perhaps the teacher did this to make the work easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2e42cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data formatting complete!\n"
     ]
    }
   ],
   "source": [
    "#2.1 Select columns subset used for training\n",
    "features = ['title', 'concatenated_tags', 'query', 'price', 'weight', 'minimum_quantity', 'category']\n",
    "products_train = products_train[features]\n",
    "\n",
    "#2.2 Clean products null and NaN occurrences. Remove only 60 lines from 38000 in total. \n",
    "products_train = products_train.dropna() \n",
    "products_train = products_train.reset_index()\n",
    "\n",
    "#2.3 Creates a k-means model for group three float columns\n",
    "def create_kmeans(products_train, ncluster=23):\n",
    "    float_columns = ['price', 'weight', 'minimum_quantity']\n",
    "    kMeansPipeline = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"kmeans\", KMeans(n_clusters=ncluster, random_state=0)),\n",
    "\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    kmeansArray_train = kMeansPipeline.fit(products_train[float_columns].dropna())\n",
    "\n",
    "    file_name = os.environ['KMEANS_PATH']\n",
    "    with open(file_name, \"wb\") as open_file:\n",
    "        pickle.dump(kMeansPipeline, open_file)\n",
    "        \n",
    "    return kMeansPipeline\n",
    "\n",
    "#2.4 Auxiliary function that tries first loading a saved kmeans predictor. If not found, creates and save it for future use.\n",
    "def load_kmeans():\n",
    "    file_name = os.environ['KMEANS_PATH']\n",
    "    try:\n",
    "        with open(file_name, 'rb') as open_file:\n",
    "            kMeansPipeline = pickle.load(open_file)\n",
    "            return kMeansPipeline\n",
    "    except:\n",
    "        return create_kmeans(products_train)\n",
    "\n",
    "#2.5 All data format steps together\n",
    "def data_format(X):\n",
    "#     textColumnsFeatures = ['title', 'concatenated_tags', 'query', 'kmeansPriceWeightMinimumQuantity']\n",
    "    kmeans = load_kmeans()\n",
    "    kmeansArray  = kmeans.predict(X[['price', 'weight', 'minimum_quantity']])\n",
    "    kmeansSeries = pd.Series(kmeansArray, name=\"kmeans\")\n",
    "    X = pd.concat([X, kmeansSeries], axis=1)\n",
    "    X['kmeansPriceWeightMinimumQuantity'] = 'grupo' + X['kmeans'].astype(str)\n",
    "#     return concatCols(X, textColumnsFeatures)\n",
    "    return X['title'] + ' ' +  X['concatenated_tags'] + ' ' +  X['query'] + ' ' +  X['kmeansPriceWeightMinimumQuantity']\n",
    "\n",
    "\n",
    "X_train = data_format(products_train)\n",
    "y_train = products_train['category']\n",
    "\n",
    "X_test  = data_format(products_test)\n",
    "y_test  = products_test['category']    \n",
    "\n",
    "print()\n",
    "print('Data formatting complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a11b1d",
   "metadata": {},
   "source": [
    "## 3. Modeling\n",
    "Specifies a model to handle the categorization problem.\n",
    "\n",
    "During experiments we used GridSearchCV class, which provided a cross-validation (5 k-fold) method to search the best set of features, pipelines and hyperparameters. Only the exploration best result is presented here.  The exploration path is documented in experiments.ipynb file.\n",
    "\n",
    "\n",
    "### 3.1. Bag-of-words feature extraction\n",
    "Most of classifiers do not work directly with **text** data. For this reason we used the CountVectorizer class that implements feature extraction of text columns. Basically it converts the text data into a matrix of token counts (bag-of-words). In addition we ended up using some extra parameters to improve the performance of the model. Below are the best combination of parameters found during parameters exploration followed by description and a possible explanation of **why** they worked.\n",
    "\n",
    "| Parameter                                 \t| Description                                                     \t| Reason why (we think) it worked                                                                                                                                                                                                               \t|\n",
    "|-------------------------------------------\t|-----------------------------------------------------------------\t|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "| binary=True                               \t| If True, all non zero counts are set to 1.                      \t| Our resulting text feature is combination of multiple text columns. This combination probably results in some duplicate words. The parameter worked because avoids CountVectorizer misinterpreting duplicated words as more valuable words.   \t|\n",
    "| max_features=None                         \t| Define no max number of features in bag-of-words.               \t| Using all words created more data. More data, better classifier.                                                                                                                                                                              \t|\n",
    "| max_df=0.5                                \t| Ignore terms that have a document frequency strictly then 0.5.  \t| It worked as avoid corpus-specific stop words. It worked well when using together the stop words list found in word clouds.                                                                                                                   \t|\n",
    "| stop_words=stop_portuguese_fromWordcoluds \t| Define a list of stop words.                                    \t| This list came from visualy inspecting the word clouds of dataset text columns. This list proved to be enough to grant the model the best scores. It is a small list that worked well together with \"max-df\" automatic stop word detection param.                                                                                                                                               \t|\n",
    "| ngram_range=(1, 2)                        \t| Means the unigrams and bigrams should be extracted.             \t| Extracting unigrams and bigrams created more data. More data, better classifier.                                                                                                                                                              \t|\n",
    "\n",
    "\n",
    "### 3.2. Choosing the classifier\n",
    "Due to our beginner level, we started exploring possible classifiers based on an example found in the scikit-learn documentation: [Sample pipeline for text feature extraction and evaluation](https://scikit-learn.org/0.15/auto_examples/grid_search_text_feature_extraction.html). This example uses an SGDClassifier.\n",
    "\n",
    "**SGDClassifier** is generic linear classifier with stochastic gradient descent (SGD) training. Seting the **loss function** parameter defines the type of classifier. The default loss function is  **hinge loss function** which defines that the classifier fits a linear **support vector machine (SVM)**. We also tried exploring other loss function and hyperparameters, but at the end, the defaults provided the best results.\n",
    "\n",
    "Only to mention, beyond this classifier, we tried some ensemble methods as RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier and XGBClassifier. Unfortunately, our lack of knowledge combined with a substantial training time increase of these models meant that we were not able to make much progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45f426ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modeling complete!\n"
     ]
    }
   ],
   "source": [
    "stop_portuquese_fromWordclouds = ['de', 'do', 'dos', 'com', 'em', 'o', 'e', 'para', 'em']\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer(binary=True, max_df=0.5, max_features=None, ngram_range=(1, 2), strip_accents=None, stop_words=stop_portuquese_fromWordclouds )),\n",
    "        (\"clf\", SGDClassifier(random_state=0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "classifier = pipeline.fit(X_train, y_train)\n",
    "\n",
    "print()\n",
    "print('Modeling complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab99f61",
   "metadata": {},
   "source": [
    "## 4. Model validation\n",
    "Generates metrics about the model accuracy (precision, recall, F1, etc.) for each category and exports them to a specified path available in the environment variable METRICS_PATH.\n",
    "\n",
    "We used three metrics:\n",
    "- **accuracy**: the measure the overall model output closeness to target data, where an accuracy score reaches its best value at 1 and worst score at 0. \n",
    "- **f1_score by category**: F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. **Precision** is intuitively the ability of the classifier not to label as positive a sample that is negative. **Recall** is intuitively the ability of the classifier to find all the positive samples.\n",
    "The formula for the F1 score is:\n",
    "\n",
    "```\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "precision = true positives / (true positives + false positives)\n",
    "\n",
    "recall =  true positives / (true positives + false negatives)\n",
    "```\n",
    "- **f1_score micro averaged**: Calculate f1_score metrics globally by counting the total true positives, false negatives and false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e63ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.962\n",
      "\n",
      "f1_score micro averaged: 0.962\n",
      "\n",
      "f1_score[Lembrancinhas]: 0.974\n",
      "f1_score[Decoração]: 0.968\n",
      "f1_score[Bebê]: 0.951\n",
      "f1_score[Papel e Cia]: 0.880\n",
      "f1_score[Outros]: 0.929\n",
      "f1_score[Bijuterias e Jóias]: 0.952\n",
      "\n",
      "Model validation complete!\n"
     ]
    }
   ],
   "source": [
    "def evaluate_test(y_true, y_predicted):\n",
    "    categories = ['Lembrancinhas', 'Decoração', 'Bebê', 'Papel e Cia', 'Outros', 'Bijuterias e Jóias']\n",
    "    accuracy = accuracy_score(y_true, y_predicted)\n",
    "    f1_score_microAveraged = f1_score(products_test['category'], y_predicted, average='micro', labels=categories)\n",
    "    f1_score_byCategories = f1_score(products_test['category'], y_predicted, average=None, labels=categories)\n",
    "    print('accuracy: %0.3f' % accuracy)\n",
    "    print('\\nf1_score micro averaged: %0.3f\\n' % f1_score_microAveraged)\n",
    "    for c, s in zip(categories , f1_score_byCategories):\n",
    "        print('f1_score[%s]: %0.3f' % (c, s) )\n",
    "        \n",
    "    file_name = os.environ['METRICS_PATH']\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write('accuracy: %0.3f\\n' % accuracy)\n",
    "        f.write('\\nf1_score micro averaged: %0.3f\\n\\n' % f1_score_microAveraged)\n",
    "        for c, s in zip(categories , f1_score_byCategories):\n",
    "            f.write('f1_score[%s]: %0.3f\\n' % (c, s) )\n",
    "\n",
    "        \n",
    "y_predicted = classifier.predict(X_test)\n",
    "evaluate_test(y_test, y_predicted)\n",
    "    \n",
    "print()\n",
    "print('Model validation complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd9d88",
   "metadata": {},
   "source": [
    "## 5. Model exportation\n",
    "Exports a candidate model to a specified path available in the environment variable MODEL_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1f0f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exportation complete!\n"
     ]
    }
   ],
   "source": [
    "file_name = os.environ['MODEL_PATH']\n",
    "with open(file_name, \"wb\") as open_file:\n",
    "    pickle.dump(classifier, open_file)\n",
    "    \n",
    "print('Model exportation complete!')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
